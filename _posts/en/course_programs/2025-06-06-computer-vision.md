---
layout: post
title: Computer vision applied to biodiversity
date:   2025-06-06
categories: [Courses]
permalink: /en/courses/vision-program
lang-ref: vision-program
---

## Description
<div style="text-align: justify">
<p>Computer vision is a field of Artificial Intelligence that aims to teach machines to "see" and interpret images in the way humans do. This has led to important technological advancements in autonomous systems such as robotics and self-driving vehicles. In this way, Biodiversity Conservation also benefits from advances in computer vision, as it enables the analysis and interpretation of complex biological data extracted from images from various sources with greater efficiency and accuracy. Learning about computer vision provides professionals with key tools to advance research as well as to enhance understanding and knowledge about species and biodiversity conservation areas.</p>

<p>This course focuses on understanding the basic concepts of computer vision from the perspective of camera trap usage, as this is one of the most important applications of computer vision in biodiversity conservation. It covers common tasks involved in these projects, such as preprocessing, feature extraction, segmentation, and object detection and recognition, for the subsequent analysis and interpretation of the data obtained from images. This course does not cover tasks such as designing camera trap systems, image acquisition, or administrative aspects of such systems.</p>

<p>This course is a response to the needs for training in various areas of Data Science identified by the Data Science Network for the Conservation of Mesoamerican Biodiversity (Redbioma). It is aimed at professionals who work in activities related to biodiversity conservation, and therefore focuses on problem-solving and the development of knowledge and skills in image processing and analysis through the use of tools, basic programming based on image processing libraries in Python, and the use of pre-trained Machine Learning models applied to image collections available in public repositories.</p>
</div>
<br>

<div>
<b>Schedule and start date</b>
<ul>
    <li>Starts on Tuesday, June 24, and ends on August 12, 2025.</li>
    <li>Classes will be held every Tuesday from 18:00 to 20:00 (GMT-6) for 8 weeks.</li>
</ul>
</div>
<br>

<div>
<b>Course type</b>
<br>
<ul>
    <li>Modality: Virtual.</li>
    <li>Theoretical/Practical: To complete the program, participants must attend more than 75% of virtual synchronous classes and achieve an average greater than or equal to 70 in evaluations.</li>
    <li>Cost: Free.</li>
</ul>
</div>
<br>

<div>
<b>Requirements</b>
<ul>
  <li>Availability of at least 16 hours during the entire program to attend eight virtual synchronous sessions. (2 hrs/class)</li>
  <li>Availability of at least 24 hours during the entire the program for completing short assignments, labs, and a final project. (3 hrs/week)</li>
  <li>Have basic knowledge of programming language Python, libraries Numpy and Pandas for data management and GeoPandas for geospatial data processing. Basic knowledge of machine learning models is also required. Preferably have passed the <i>Introduction to Python for Data Science</i> course, and optionally the <i>Machine Learning</i> course offered by Redbioma.</li>
  <li>Fill out the form to <a href="https://forms.gle/gq98uQN32xz9uBx87">Participate in Redbioma activities</a> (previously circulated, please fill out only once).</li>
</ul>
</div>
<br>

## Registration form
Link: [Registration for Computer vision applied to biodiversity](https://forms.gle/tCovPqgZxrN5h3qR9)

<br>

## Objectives

#### General
<div style="text-align: justify">
<p>The development of image processing and analysis skills commonly associated with camera trap systems, using common tools, Python programming and machine learning techniques.</p>
</div>
<br>

#### Specific
<div style="text-align: justify"><ul>
    <li>Build an understanding of the general concepts of computer vision related to image processing and analysis, as well as those specific to different areas of Biodiversity Conservation.</li>
    <li>Identify and distinguish between common image processing and analysis tasks.</li>
    <li>Use popular open-access tools for the processing and analysis of biodiversity images.</li>
    <li>Implement basic routines using Python libraries aimed at manipulating images obtained from camera trap systems.</li>
    <li>Understand and apply basic machine learning models for the processing and analysis of camera trap images.</li>
</ul></div>
<br>

## Course methodology

<div style="text-align: justify">
<p>The course methodology will be based on active and collaborative learning, through problem-solving activities in labs, research work, and flipped classroom strategies, among other techniques. The goal is to guide students in strengthening their ability to conduct research, use public data collections, critically analyze scientific articles, and apply new concepts based on their prior knowledge and the course content.</p>
<p>The course program is both theoretical and practical, where participants will apply theoretical knowledge through case studies, group discussions, lab sessions, and a final project.</p>
<b>Importante:</b>
<ul>
    <li>All synchronous sessions will be recorded and published on the Redbioma website.</li>
    <li>Final research projects will be published on the Redbioma website.</li>
</ul>
</div>
<br>

## Program content
<div style="text-align: justify">
  <ol>
    <b><li>Fundamentals of computer vision</li></b>
    <ol type="i">
        <li>Images and basic concepts.</li>
        <li>Basic tasks related to computer vision.</li>
        <li>Popular computer vision models based on Machine Learning and Deep Learning.</li>
        <li>Basic tools and commonly used Python libraries.</li>
    </ol>
    <b><li>Introduction to image processing</li></b>
    <ol type="i">
        <li>Description of common image processing tasks.</li>
          <ol type="i">
            <li>Preprocessing.</li>
            <li>Enhancement.</li>
            <li>Segmentation.</li>
            <li>Feature extraction.</li>
          </ol>
        <li>Specific tools for biodiversity image processing.</li>
        <li>Implementation of common image processing tasks in Python.</li>
        <li>Application of pretrained models based on supervised learning techniques.</li>
    </ol>
    <b><li>Introduction to image analysis</li></b>
    <ol type="i">
        <li>Description of common image analysis tasks.</li>
          <ol type="i">
            <li>Annotation.</li>
            <li>Image classification.</li>
            <li>Motion detection.</li>
            <li>Anomaly detection.</li>
          </ol>
        <li>Specific tools for biodiversity image analysis.</li>
        <li>Implementation of common image analysis tasks in Python.</li>
        <li>Application of pretrained models based on supervised learning techniques.</li>
    </ol>
    <b><li>Camera trap systems</li></b>
    <ol type="i">
        <li>Definitions and basic concepts.</li>
        <li>Data standards: CamTrap DP.</li>
        <li>Camera trap image processing.</li>
          <ol type="i">
            <li>Organization.</li>
            <li>Cleaning: duplicates and false triggers.</li>
            <li>Labeling.</li>
            <li>Tools and models based on machine learning.</li>
          </ol>
        <li>Camera trap image analysis.</li>
          <ol type="i">
            <li>Species recognition.</li>
            <li>Species occupancy models.</li>
            <li>Activity patterns.</li>
            <li>Tools and models based on machine learning.</li>
          </ol>
    </ol>
    <b><li>Final reflections</li></b>
    <ol type="i">
        <li>Challenges and risks of using images in biodiversity conservation..</li>
        <li>OpOpportunities for using camera traps in the professional context of participants.</li>
    </ol>
</ol>
</div>
<br>


## Evaluation
<div style="text-align: justify">
<p>Students will complete short assignments, labs and a final project. Evaluation items are as follows:</p>
</div>
<br>

<table style="width:30%">
  <tr>
    <th>Item</th>
    <th>Value (%)</th>
  </tr>
  <tr>
    <td>Short assignments</td>
    <td>30</td>
  </tr>
  <tr>
    <td>Labs</td>
    <td>30</td>
  </tr>
  <tr>
    <td>Class participation</td>
    <td>10</td>
  </tr>
  <tr>
    <td>Final project</td>
    <td>30</td>
  </tr>
  <tr>
    <td>Total</td>
    <td>100</td>
  </tr>
</table>

<div style="text-align: justify">
<p>Short assignments will help reinforce the knowledge and skills acquired during the synchronous sessions and will allow participants to explore topics that complement the course content. Labs will be conducted during the synchronous sessions using a variety of tools and environments. Image collections will be made available prior to each session. Class participation includes the student’s oral contributions as well as the satisfactory completion of the activities planned for each session. The final project consists of creating a conference-style poster presenting the results of implementing image processing and analysis tasks for a common use case involving a camera trap system.</p>
<p>Each evaluation will have a previously established submission date. The submission deadline is 11:45 PM (GMT-6). Late submissions will not be accepted. All assignments must be submitted via Google Classroom.</p>
</div>
<br>

## Class schedule

<table style="width:60%">
  <tr>
    <th>Class</th>
    <th>Week</th>
  </tr>
  <tr>
    <td>Fundamentals of computer vision</td>
    <td>1</td>
  </tr>
  <tr>
    <td>Introduction to image processing</td>
    <td>1-2</td>
  </tr>
  <tr>
    <td>Introduction to image analysis</td>
    <td>3-4</td>
  </tr>
  <tr>
    <td>Processing and analysis of camera trap systems</td>
    <td>5-8</td>
  </tr>
  <tr>
    <td>Final reflections and presentation of posters</td>
    <td>8</td>
  </tr>
</table>
<br>

## Resources

- Python libraries: Pillow, OpenCV, scikit-image, TensorFlow, Keras.
- Artificial Intelligence-based platforms: Wildlife Insights (WI), Machine Learning for Wildlife Image Classification (MLWIC2), MegaDetector (MD), and Conservation AI.
- Image processing and analysis tools: Timelapse, Camelot.
- Public image collections obtained from repositories such as GBIF and CalTech.
- GBIF Webinars.

<br>

## References

1. Duggan, M. T., Groleau, M. F., Shealy, E. P., Self, L. S., Utter, T. E., Waller, M. M., Hall, B. C., Stone, C. G., Anderson, L. L., & Mousseau, T. A. (2021). An approach to rapid processing of camera trap images with minimal human input. *Ecology and Evolution, 11*(17), 12051–12063. [External link](https://doi.org/10.1002/ece3.7970)

2. Egloff W, Agosti D, Kishor P, Patterson D, Miller J (2017). Copyright and the Use of Images as Biodiversity Data. *Research Ideas and Outcomes 3*: e12502. [External link](https://doi.org/10.3897/rio.3.e12502)

3. Fennell, M., Beirne, C., & Burton, A. C. (2022). Use of object detection in camera trap image identification: Assessing a method to rapidly and accurately classify human and animal detections for research and application in recreation ecology. *Global Ecology and Conservation, 35*, e02104. [External link](https://doi.org/10.1016/j.gecco.2022.e02104)

4. Fergus, P., Chalmers, C., Longmore, S., & Wich, S. (2024). Harnessing Artificial Intelligence for Wildlife Conservation. *Conservation, 4*(4), 685–702. [External link](https://doi.org/10.3390/conservation4040041)

5. Greenberg, S. (n.d.). *Automated Image Recognition for Wildlife Camera Traps: Making it Work for You 1*. Retrieved June 4, 2025, from [External link](https://wildcams.ca/site/assets/files/1389/2020-08-greenberg-imagerecognitioncameratraps_updated.pdf)

6. Hilton, M. L., Yamane, M. T., & Knezevich, L. M. (2022). *An Image Processing Pipeline for Camera Trap Time-Lapse Recordings*. ArXiv.org. [External link](https://arxiv.org/abs/2206.05159)

7. Leorna, S., & Brinkman, T. (2022). Human vs. machine: Detecting wildlife in camera trap images. *Ecological Informatics, 72*, 101876. [External link](https://doi.org/10.1016/j.ecoinf.2022.101876)

8. Pantazis, O., Bevan, P., Pringle, H., Ferreira, G. B., Ingram, D. J., Madsen, E., Thomas, L., Raj, T. D., Silwal, T., Rayamajhi, S., Brostow, G., Aodha, M., & Jones, K. E. (2024). *Deep learning-based ecological analysis of camera trap images is impacted by training data quality and size*. ArXiv.org. [External link](https://arxiv.org/abs/2408.14348)

9. Vélez, J., McShea, W., Shamon, H., Castiblanco-Camacho, P. J., Tabak, M. A., Chalmers, C., Fergus, P., & Fieberg, J. (2022). An evaluation of platforms for processing camera-trap data using artificial intelligence. *Methods in Ecology and Evolution, 14*(2), 459–477. [External link](https://doi.org/10.1111/2041-210x.1404)

<br>

## Contacts

<table style="width:40%">
  <tr>
    <th>Professors</th>
    <th>Email</th>
  </tr>
  <tr>
    <td>Instructor: Emilia Zeledón</td>
    <td><a href="mailto:emilia.zeledon@gmail.com">emilia.zeledon@gmail.com</a></td>
  </tr>
  <tr>
    <td>María Auxiliadora Mora</td>
    <td><a href="mailto:maria.mora@itcr.ac.cr">maria.mora@itcr.ac.cr</a></td>
  </tr>
</table>
